{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Неоднозначность.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olgasem10/Compling-tasks/blob/master/%D0%9D%D0%B5%D0%BE%D0%B4%D0%BD%D0%BE%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%BE%D1%81%D1%82%D1%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DDO2y9Z4WDb",
        "colab_type": "code",
        "outputId": "d9254d4e-8e91-4ab9-a097-38dc2b39c9bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import adagram\n",
        "import spacy\n",
        "from lxml import html\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from string import punctuation\n",
        "import json, os\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "morph = MorphAnalyzer()\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian'))\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "def opt_norm(words):\n",
        "    uniq = list(set(words))\n",
        "    norm_uniq = {word:morph.parse(word)[0].normal_form for word in uniq}\n",
        "    norm_words = [norm_uniq.get(word) for word in words]\n",
        "    norm_words = [word for word in norm_words if word]\n",
        "    return norm_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cLtTbW84WD0",
        "colab_type": "text"
      },
      "source": [
        "Обучаем адаграм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_ffEaRU4WD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open('corpus_ng.txt', encoding = 'utf-8').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2xYVyTz4WEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm_corp = opt_norm(tokenize(corpus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuvU-Fhi4WEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f = open('corpus.txt', 'w', encoding = 'utf-8')\n",
        "f.write(' '.join(norm_corp))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JD3Hb_KNgPO_",
        "outputId": "4cdea7db-70ba-4404-c76f-79e37c5e9bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!adagram-train corpus.txt mod1.pkl --dim 300"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] 2020-02-28 11:19:32,850 Building dictionary...\n",
            "[INFO] 2020-02-28 11:30:00,700 Done! 25010 words.\n",
            "[INFO] 2020-02-28 11:40:35,728 1.24% -8.3443 0.0247 1.1/2.0 0.10 kwords/sec\n",
            "[INFO] 2020-02-28 11:40:49,089 2.49% -8.2633 0.0244 1.1/2.0 4.79 kwords/sec\n",
            "[INFO] 2020-02-28 11:41:01,878 3.73% -8.1649 0.0241 1.1/2.0 5.00 kwords/sec\n",
            "[INFO] 2020-02-28 11:41:14,301 4.97% -8.0576 0.0238 1.1/2.0 5.15 kwords/sec\n",
            "[INFO] 2020-02-28 11:41:26,021 6.22% -7.9466 0.0234 1.1/2.0 5.46 kwords/sec\n",
            "[INFO] 2020-02-28 11:41:37,613 7.46% -7.8346 0.0231 1.1/3.0 5.52 kwords/sec\n",
            "[INFO] 2020-02-28 11:41:48,974 8.70% -7.7229 0.0228 1.2/3.0 5.63 kwords/sec\n",
            "[INFO] 2020-02-28 11:42:00,271 9.95% -7.6131 0.0225 1.2/3.0 5.67 kwords/sec\n",
            "[INFO] 2020-02-28 11:42:11,639 11.19% -7.5062 0.0222 1.2/3.0 5.63 kwords/sec\n",
            "[INFO] 2020-02-28 11:42:22,803 12.43% -7.4029 0.0219 1.3/4.0 5.73 kwords/sec\n",
            "[INFO] 2020-02-28 11:42:34,265 13.68% -7.3039 0.0216 1.3/4.0 5.58 kwords/sec\n",
            "[INFO] 2020-02-28 11:42:45,640 14.92% -7.2098 0.0213 1.3/4.0 5.63 kwords/sec\n",
            "[INFO] 2020-02-28 11:42:57,007 16.16% -7.1207 0.0210 1.4/5.0 5.63 kwords/sec\n",
            "[INFO] 2020-02-28 11:43:08,366 17.41% -7.0367 0.0206 1.4/5.0 5.63 kwords/sec\n",
            "[INFO] 2020-02-28 11:43:19,528 18.65% -6.9579 0.0203 1.4/5.0 5.73 kwords/sec\n",
            "[INFO] 2020-02-28 11:43:30,704 19.89% -6.8841 0.0200 1.4/5.0 5.73 kwords/sec\n",
            "[INFO] 2020-02-28 11:43:41,813 21.14% -6.8152 0.0197 1.4/5.0 5.76 kwords/sec\n",
            "[INFO] 2020-02-28 11:43:52,963 22.38% -6.7507 0.0194 1.4/5.0 5.74 kwords/sec\n",
            "[INFO] 2020-02-28 11:44:04,046 23.62% -6.6906 0.0191 1.4/5.0 5.77 kwords/sec\n",
            "[INFO] 2020-02-28 11:44:15,051 24.87% -6.6343 0.0188 1.4/5.0 5.82 kwords/sec\n",
            "[INFO] 2020-02-28 11:44:25,864 26.11% -6.5817 0.0185 1.4/5.0 5.92 kwords/sec\n",
            "[INFO] 2020-02-28 11:44:36,849 27.35% -6.5324 0.0182 1.4/5.0 5.83 kwords/sec\n",
            "[INFO] 2020-02-28 11:44:47,740 28.60% -6.4862 0.0179 1.4/5.0 5.88 kwords/sec\n",
            "[INFO] 2020-02-28 11:44:58,650 29.84% -6.4428 0.0175 1.4/5.0 5.87 kwords/sec\n",
            "[INFO] 2020-02-28 11:45:09,444 31.08% -6.4019 0.0172 1.4/5.0 5.93 kwords/sec\n",
            "[INFO] 2020-02-28 11:45:20,043 32.33% -6.3634 0.0169 1.4/5.0 6.04 kwords/sec\n",
            "[INFO] 2020-02-28 11:45:30,753 33.57% -6.3270 0.0166 1.4/5.0 5.98 kwords/sec\n",
            "[INFO] 2020-02-28 11:45:41,392 34.81% -6.2926 0.0163 1.4/5.0 6.02 kwords/sec\n",
            "[INFO] 2020-02-28 11:45:52,087 36.06% -6.2600 0.0160 1.4/5.0 5.98 kwords/sec\n",
            "[INFO] 2020-02-28 11:46:02,717 37.30% -6.2290 0.0157 1.4/5.0 6.02 kwords/sec\n",
            "[INFO] 2020-02-28 11:46:13,449 38.54% -6.1995 0.0154 1.4/5.0 5.96 kwords/sec\n",
            "[INFO] 2020-02-28 11:46:23,885 39.79% -6.1714 0.0151 1.4/5.0 6.13 kwords/sec\n",
            "[INFO] 2020-02-28 11:46:34,468 41.03% -6.1446 0.0147 1.4/5.0 6.05 kwords/sec\n",
            "[INFO] 2020-02-28 11:46:45,014 42.27% -6.1190 0.0144 1.4/5.0 6.07 kwords/sec\n",
            "[INFO] 2020-02-28 11:46:55,556 43.52% -6.0945 0.0141 1.4/5.0 6.07 kwords/sec\n",
            "[INFO] 2020-02-28 11:47:06,117 44.76% -6.0710 0.0138 1.4/5.0 6.06 kwords/sec\n",
            "[INFO] 2020-02-28 11:47:16,598 46.00% -6.0484 0.0135 1.4/5.0 6.11 kwords/sec\n",
            "[INFO] 2020-02-28 11:47:27,011 47.25% -6.0267 0.0132 1.4/5.0 6.15 kwords/sec\n",
            "[INFO] 2020-02-28 11:47:37,538 48.49% -6.0058 0.0129 1.4/5.0 6.08 kwords/sec\n",
            "[INFO] 2020-02-28 11:47:48,178 49.73% -5.9856 0.0126 1.4/5.0 6.01 kwords/sec\n",
            "[INFO] 2020-02-28 11:47:58,684 50.98% -5.9662 0.0123 1.4/5.0 6.09 kwords/sec\n",
            "[INFO] 2020-02-28 11:48:09,218 52.22% -5.9474 0.0119 1.4/5.0 6.08 kwords/sec\n",
            "[INFO] 2020-02-28 11:48:19,615 53.46% -5.9292 0.0116 1.4/5.0 6.16 kwords/sec\n",
            "[INFO] 2020-02-28 11:48:30,037 54.71% -5.9116 0.0113 1.4/5.0 6.14 kwords/sec\n",
            "[INFO] 2020-02-28 11:48:40,461 55.95% -5.8946 0.0110 1.4/5.0 6.14 kwords/sec\n",
            "[INFO] 2020-02-28 11:48:50,952 57.19% -5.8780 0.0107 1.4/5.0 6.10 kwords/sec\n",
            "[INFO] 2020-02-28 11:49:01,334 58.44% -5.8620 0.0104 1.4/5.0 6.16 kwords/sec\n",
            "[INFO] 2020-02-28 11:49:11,821 59.68% -5.8463 0.0101 1.4/5.0 6.10 kwords/sec\n",
            "[INFO] 2020-02-28 11:49:22,052 60.92% -5.8311 0.0098 1.4/5.0 6.26 kwords/sec\n",
            "[INFO] 2020-02-28 11:49:32,381 62.17% -5.8163 0.0095 1.4/5.0 6.20 kwords/sec\n",
            "[INFO] 2020-02-28 11:49:42,725 63.41% -5.8019 0.0091 1.4/5.0 6.19 kwords/sec\n",
            "[INFO] 2020-02-28 11:49:53,100 64.65% -5.7878 0.0088 1.4/5.0 6.17 kwords/sec\n",
            "[INFO] 2020-02-28 11:50:03,473 65.90% -5.7741 0.0085 1.4/5.0 6.17 kwords/sec\n",
            "[INFO] 2020-02-28 11:50:13,891 67.14% -5.7607 0.0082 1.4/5.0 6.14 kwords/sec\n",
            "[INFO] 2020-02-28 11:50:24,090 68.38% -5.7476 0.0079 1.4/5.0 6.28 kwords/sec\n",
            "[INFO] 2020-02-28 11:50:34,403 69.63% -5.7347 0.0076 1.4/5.0 6.21 kwords/sec\n",
            "[INFO] 2020-02-28 11:50:44,792 70.87% -5.7222 0.0073 1.4/5.0 6.16 kwords/sec\n",
            "[INFO] 2020-02-28 11:50:55,210 72.11% -5.7099 0.0070 1.4/5.0 6.14 kwords/sec\n",
            "[INFO] 2020-02-28 11:51:05,610 73.36% -5.6978 0.0067 1.4/5.0 6.15 kwords/sec\n",
            "[INFO] 2020-02-28 11:51:15,953 74.60% -5.6860 0.0064 1.4/5.0 6.19 kwords/sec\n",
            "[INFO] 2020-02-28 11:51:26,217 75.84% -5.6744 0.0060 1.4/5.0 6.24 kwords/sec\n",
            "[INFO] 2020-02-28 11:51:36,528 77.09% -5.6630 0.0057 1.4/5.0 6.21 kwords/sec\n",
            "[INFO] 2020-02-28 11:51:46,871 78.33% -5.6518 0.0054 1.4/5.0 6.19 kwords/sec\n",
            "[INFO] 2020-02-28 11:51:57,172 79.57% -5.6409 0.0051 1.4/5.0 6.21 kwords/sec\n",
            "[INFO] 2020-02-28 11:52:07,520 80.82% -5.6301 0.0048 1.4/5.0 6.18 kwords/sec\n",
            "[INFO] 2020-02-28 11:52:17,737 82.06% -5.6195 0.0045 1.4/5.0 6.26 kwords/sec\n",
            "[INFO] 2020-02-28 11:52:27,994 83.30% -5.6091 0.0042 1.4/5.0 6.24 kwords/sec\n",
            "[INFO] 2020-02-28 11:52:38,253 84.55% -5.5988 0.0039 1.4/5.0 6.24 kwords/sec\n",
            "[INFO] 2020-02-28 11:52:48,546 85.79% -5.5887 0.0036 1.4/5.0 6.22 kwords/sec\n",
            "[INFO] 2020-02-28 11:52:58,982 87.03% -5.5788 0.0032 1.4/5.0 6.13 kwords/sec\n",
            "[INFO] 2020-02-28 11:53:09,298 88.28% -5.5690 0.0029 1.4/5.0 6.20 kwords/sec\n",
            "[INFO] 2020-02-28 11:53:19,410 89.52% -5.5594 0.0026 1.4/5.0 6.33 kwords/sec\n",
            "[INFO] 2020-02-28 11:53:29,674 90.76% -5.5500 0.0023 1.4/5.0 6.24 kwords/sec\n",
            "[INFO] 2020-02-28 11:53:39,909 92.01% -5.5407 0.0020 1.4/5.0 6.25 kwords/sec\n",
            "[INFO] 2020-02-28 11:53:50,266 93.25% -5.5316 0.0017 1.4/5.0 6.18 kwords/sec\n",
            "[INFO] 2020-02-28 11:54:00,531 94.49% -5.5226 0.0014 1.4/5.0 6.23 kwords/sec\n",
            "[INFO] 2020-02-28 11:54:10,839 95.74% -5.5138 0.0011 1.4/5.0 6.21 kwords/sec\n",
            "[INFO] 2020-02-28 11:54:20,894 96.98% -5.5052 0.0008 1.4/5.0 6.37 kwords/sec\n",
            "[INFO] 2020-02-28 11:54:31,112 98.22% -5.4967 0.0004 1.4/5.0 6.26 kwords/sec\n",
            "[INFO] 2020-02-28 11:54:41,280 99.47% -5.4884 0.0001 1.4/5.0 6.29 kwords/sec\n",
            "[INFO] 2020-02-28 11:54:45,708 100.00% -5.4849 0.0000 1.4/5.0 6.20 kwords/sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwG5YB5R4WE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vm = adagram.VectorModel.load(\"mod1.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7alLIubn4WHQ",
        "colab_type": "text"
      },
      "source": [
        "## Адаграм в определении перефразирования"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chmzHSUm4WHV",
        "colab_type": "code",
        "outputId": "bc24a5c5-d7ef-409b-c18d-e16fb4f977fa",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('paraphraser_norm.csv')\n",
        "data.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text_1</th>\n",
              "      <th>text_2</th>\n",
              "      <th>text_1_norm</th>\n",
              "      <th>text_2_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
              "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
              "      <td>полицейский разрешать стрелять на поражение по...</td>\n",
              "      <td>полиция мочь разрешать стрелять по хулиган с т...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
              "      <td>Правила внесудебного проникновения полицейских...</td>\n",
              "      <td>право полицейский на проникновение в жилище ре...</td>\n",
              "      <td>правило внесудебный проникновение полицейский ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                             text_1  \\\n",
              "0      0  Полицейским разрешат стрелять на поражение по ...   \n",
              "1      0  Право полицейских на проникновение в жилище ре...   \n",
              "\n",
              "                                              text_2  \\\n",
              "0  Полиции могут разрешить стрелять по хулиганам ...   \n",
              "1  Правила внесудебного проникновения полицейских...   \n",
              "\n",
              "                                         text_1_norm  \\\n",
              "0  полицейский разрешать стрелять на поражение по...   \n",
              "1  право полицейский на проникновение в жилище ре...   \n",
              "\n",
              "                                         text_2_norm  \n",
              "0  полиция мочь разрешать стрелять по хулиган с т...  \n",
              "1  правило внесудебный проникновение полицейский ...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-M7kiG94WHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_words_in_context(words, window=3):\n",
        "    contexts = []\n",
        "    for i, word in enumerate(words):\n",
        "        word_and_cont = [word]\n",
        "        word_cont = []\n",
        "        for j in range(i-window, i+window+1):\n",
        "            if j in range(0, len(words)) and j != i:\n",
        "                word_cont.append(words[j])\n",
        "        word_and_cont.append(word_cont)\n",
        "        contexts.append(word_and_cont)\n",
        "    return contexts   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrKznQ-d4WJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dis_get_embedding(text, model, dim):\n",
        "    words = text.split()\n",
        "    word2context = get_words_in_context(words, 3)\n",
        "    vectors = np.zeros((len(word2context), dim))\n",
        "    for i, (word, context) in enumerate(word2context):\n",
        "        try:\n",
        "            num = model.disambiguate(word, context).argmax()\n",
        "            vec = model.sense_vector(word, num)\n",
        "            vectors[i] = vec\n",
        "        except (KeyError, ValueError):\n",
        "            continue\n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cgu0Zk04WJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dim = 300\n",
        "X_text_1 = np.zeros((len(data['text_1_norm']), dim))\n",
        "X_text_2 = np.zeros((len(data['text_2_norm']), dim))\n",
        "\n",
        "for i, text in enumerate(data['text_1_norm'].values):\n",
        "    X_text_1[i] = dis_get_embedding(text, vm, dim)\n",
        "    \n",
        "for i, text in enumerate(data['text_2_norm'].values):\n",
        "    X_text_2[i] = dis_get_embedding(text, vm, dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6WuE6zg4WJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text = np.concatenate([X_text_1, X_text_2], axis=1)\n",
        "y = data['label'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcdjgbJM4WKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_leaf=10,\n",
        "                             class_weight='balanced')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBhOwLl84WKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = cross_val_score(clf, X_text, y, cv=10, scoring='f1_macro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0-kynzc4WKi",
        "colab_type": "code",
        "outputId": "13fb803b-a5ae-4876-a6c7-9f9c961367e3",
        "colab": {}
      },
      "source": [
        "print('Macro f1-score')\n",
        "for score in scores:\n",
        "    print(score)\n",
        "print('\\nMean score')\n",
        "print(np.mean(scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Macro f1-score\n",
            "0.44059492105877524\n",
            "0.42692947690192756\n",
            "0.4631405353209865\n",
            "0.47681058107372326\n",
            "0.4549289538300528\n",
            "0.43898328018701854\n",
            "0.37050355401342916\n",
            "0.3174813419357707\n",
            "0.31467056043142044\n",
            "0.39399778358106735\n",
            "\n",
            "Mean score\n",
            "0.4098040988334172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTs1MvMC4WKw",
        "colab_type": "text"
      },
      "source": [
        "## Алгоритм Леска"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMN8tImh4WLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_wsd = []\n",
        "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
        "for sent in corpus:\n",
        "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mw8mrxj4WNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lesk(word, context):\n",
        "    defs = [set(synset.definition().split()) for synset in wn.synsets(word)]\n",
        "    inters = [len(context.intersection(one_def)) for one_def in defs]\n",
        "    number = inters.index(max(inters))\n",
        "    return number    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pqPTziW4WNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "umbig_words = 0\n",
        "right_res = 0\n",
        "for sent in corpus_wsd:\n",
        "    clean_sent = [word[1] for word in sent]\n",
        "    word2context = get_words_in_context(clean_sent, 3)\n",
        "    for i, word in enumerate(sent):\n",
        "        if word[0]:\n",
        "            umbig_words += 1\n",
        "            context = set(word2context[i][1])\n",
        "            best = lesk(word[1], context)\n",
        "            if wn.synsets(word[1])[best] == wn.lemma_from_key(word[0]).synset():\n",
        "                right_res += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRL6To5-mJyI",
        "colab_type": "code",
        "outputId": "00f11fab-8490-4781-8f7e-72c2af566aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "right_res/umbig_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3839308415967455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qy8f0Lgg4WOC",
        "colab_type": "text"
      },
      "source": [
        "Для того чтобы улучшить алгоритм, попробуем вместо пересечения множеств учитывать косинусное расстояние между векторами контекста и возможных определений слова"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GB_2cz1uWNwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('model1.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek3S_NGX4WOw",
        "colab_type": "code",
        "outputId": "e2d50a28-7722-41ce-eef3-d392d1e55525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model.most_similar('good')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bad', 0.7223736047744751),\n",
              " ('excellent', 0.7079846858978271),\n",
              " ('decent', 0.6534404754638672),\n",
              " ('poor', 0.5662919282913208),\n",
              " ('better', 0.56507408618927),\n",
              " ('nice', 0.5626944303512573),\n",
              " ('luck', 0.550661563873291),\n",
              " ('wonderful', 0.5491265058517456),\n",
              " ('fortunate', 0.5324512720108032),\n",
              " ('pretty', 0.5311379432678223)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKoKVnFb4WO8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding(text, model, dim):\n",
        "    vectors = np.zeros((len(text), dim))\n",
        "    for i,word in enumerate(text):\n",
        "        try:\n",
        "            v = model[word]\n",
        "            vectors[i] = v\n",
        "        except (KeyError, ValueError):\n",
        "            continue\n",
        "    \n",
        "    if vectors.any():\n",
        "        vector = np.average(vectors, axis=0)\n",
        "    else:\n",
        "        vector = np.zeros((dim))\n",
        "    \n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJQ194eJ4WPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def en_tokenize(text):\n",
        "    words = [word.strip(punct).lower() for word in text.split()]\n",
        "    words = [word for word in words if word]\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdGXdG8-k7Ug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "def spacy_lemm(sent):\n",
        "  sent = ' '.join(tokenize(sent))\n",
        "  doc = nlp(sent)\n",
        "  out_sent = [w.lemma_  for w in doc if w.lemma_ !='-PRON-']\n",
        "  return(out_sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTMfvnh5hLmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "umbig_words = 0\n",
        "right_res = 0\n",
        "for sent in corpus_wsd[:100]:\n",
        "    clean_sent = [word[1] for word in sent]\n",
        "    word2context = get_words_in_context(clean_sent, 3)\n",
        "    for i, word in enumerate(sent):\n",
        "        if word[0]:\n",
        "            umbig_words += 1\n",
        "            context = ' '.join(word2context[i][1])\n",
        "            norm_context = spacy_lemm(context)\n",
        "            context_vec = get_embedding(norm_context, model, 300)\n",
        "            defs = [get_embedding(spacy_lemm(synset.definition()), model, 300) for synset in wn.synsets(word[1])]\n",
        "            dist = [cosine_distances([context_vec], [def_vec]) for def_vec in defs]\n",
        "            number = dist.index(min(dist))\n",
        "            if wn.synsets(word[1])[number] == wn.lemma_from_key(word[0]).synset():\n",
        "                right_res += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOo6r5uaD2IU",
        "colab_type": "code",
        "outputId": "ed8161b2-5823-4507-a8e4-b0847b648be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "right_res/umbig_words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4252577319587629"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tyb6J-EEqAQ",
        "colab_type": "text"
      },
      "source": [
        "На больших объемах очень долго считается, возможно что на всем корпусе результат окажется примерно таким же как в простом алгоритме Леска"
      ]
    }
  ]
}