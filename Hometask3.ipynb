{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hometask3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olgasem10/Compling-tasks/blob/master/Hometask3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6uFR8kfSyQ2",
        "colab_type": "text"
      },
      "source": [
        "#Алгоритм Symspell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeHb6zd2ElzK",
        "colab_type": "code",
        "outputId": "a9763bbb-798d-430e-a818-232a709d5d56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os, re\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FuHedP9A3bX",
        "colab_type": "code",
        "outputId": "75a6693e-8cc1-4924-b544-b502756351e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/olgasem10/Compling-tasks/master/corpus_10000.txt\n",
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n",
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-21 09:52:54--  https://raw.githubusercontent.com/olgasem10/Compling-tasks/master/corpus_10000.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23882605 (23M) [text/plain]\n",
            "Saving to: ‘corpus_10000.txt.1’\n",
            "\n",
            "\rcorpus_10000.txt.1    0%[                    ]       0  --.-KB/s               \rcorpus_10000.txt.1  100%[===================>]  22.78M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-11-21 09:52:54 (188 MB/s) - ‘corpus_10000.txt.1’ saved [23882605/23882605]\n",
            "\n",
            "--2019-11-21 09:52:55--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120672 (118K) [text/plain]\n",
            "Saving to: ‘correct_sents.txt.1’\n",
            "\n",
            "correct_sents.txt.1 100%[===================>] 117.84K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-21 09:52:55 (5.33 MB/s) - ‘correct_sents.txt.1’ saved [120672/120672]\n",
            "\n",
            "--2019-11-21 09:52:57--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123167 (120K) [text/plain]\n",
            "Saving to: ‘sents_with_mistakes.txt.1’\n",
            "\n",
            "sents_with_mistakes 100%[===================>] 120.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-21 09:52:57 (5.60 MB/s) - ‘sents_with_mistakes.txt.1’ saved [123167/123167]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu9oNnfDDtJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L1yQUixD4GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB6iWIN8EPps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    normalized_text = [(word.strip(punctuation)) for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTcZn-FVEdCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_10000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2zlqWfNE82t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter()\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5762-YNgDBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = sum(WORDS.values())\n",
        "def P(word, N=N): \n",
        "    return WORDS[word] / N"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_qGMi94TIZl",
        "colab_type": "text"
      },
      "source": [
        "Зададим функцию для генерации удалений:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv3pgVQHdmdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edits(word):\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:] for L, R in splits if R]\n",
        "    return(deletes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7P7HD9qbTQUt",
        "colab_type": "text"
      },
      "source": [
        "Создадим словарь, ключами в котором будут слова с удалениями, а значениями - списки всех возможных исходных слов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YLR3N8N0Yype",
        "colab": {}
      },
      "source": [
        "voc = {}\n",
        "for sent in corpus:\n",
        "  for word in sent:\n",
        "    for dlt in edits(word):\n",
        "      if dlt in voc:\n",
        "        if not word in voc[dlt]:\n",
        "            voc[dlt].append(word)\n",
        "      else:\n",
        "        voc[dlt] = [word]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTC9jeFGUBeB",
        "colab_type": "text"
      },
      "source": [
        "Зададим функцию, возвращающую слово с исправленной ошибкой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwQhWvNJJyFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correction(word):\n",
        "  if word in WORDS:\n",
        "    corr = word\n",
        "  else:\n",
        "    pot_words = []\n",
        "    if word in voc:\n",
        "      pot_words.extend(voc[word])\n",
        "    cand = edits(word)\n",
        "    for cnd in cand:\n",
        "      if cnd in WORDS:\n",
        "        pot_words.append(cnd)\n",
        "      if cnd in voc:\n",
        "        pot_words.extend(voc[cnd])\n",
        "    if pot_words:\n",
        "      corr = max(pot_words, key = P)\n",
        "    else:\n",
        "      corr = word\n",
        "  return corr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xooP2qgNUfsX",
        "colab_type": "text"
      },
      "source": [
        "Оценим результат"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUZKMe9Vf3cC",
        "colab_type": "code",
        "outputId": "696efb71-ee88-4ca9-ff56-c8822685d520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], correction(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        \n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1\n",
        "        \n",
        "    if not i % 100:\n",
        "        print(i)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiwfXJDviwdl",
        "colab_type": "code",
        "outputId": "31205dee-c7e2-4d92-ca70-d44df72580b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7328671328671329\n",
            "0.4481964696853415\n",
            "0.22453198575858505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtERYf2ZxNf9",
        "colab_type": "text"
      },
      "source": [
        "#С учетом триграмной модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKZ5HO7sxTih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_IEHZm6V8XA",
        "colab_type": "text"
      },
      "source": [
        "Зададим функцию для выявления n-грамм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HF5-giofxs4V",
        "colab": {}
      },
      "source": [
        "def ngrammer(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FTytxKMCx3MK",
        "colab": {}
      },
      "source": [
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "for sentence in sentences:\n",
        "    bigrams.update(ngrammer(sentence, 2))\n",
        "    trigrams.update(ngrammer(sentence, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pOzQo6lWHf0",
        "colab_type": "text"
      },
      "source": [
        "Зададим функцию, возвращающую список всех возможных исходных слов для слова с ошибкой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ7-rQhFPdQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pot_replacements(word):\n",
        "  pot_words = []\n",
        "  if word in voc:\n",
        "   pot_words.extend(voc[word])\n",
        "  cand = edits(word)\n",
        "  for cnd in cand:\n",
        "    if cnd in WORDS:\n",
        "      pot_words.append(cnd)\n",
        "    if cnd in voc:\n",
        "      pot_words.extend(voc[cnd])\n",
        "  return pot_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG2SUWksWPpu",
        "colab_type": "text"
      },
      "source": [
        "Зададим функцию, добавляющую к каждому слову из списка его вероятность"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR_s4_HM1oq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pot_replacements_with_probs(words):\n",
        "  a = [(word, P(word)) for word in words]\n",
        "  with_probs = sorted(a, key=lambda x: -x[1])\n",
        "  return with_probs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJWUfAVWY9Z",
        "colab_type": "text"
      },
      "source": [
        "Оценим результат"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjwq18jM0t_P",
        "colab_type": "code",
        "outputId": "85045162-8ae2-432e-e23d-54fb21c3cb8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "mistakes = []\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "for i in range(len(true)):\n",
        "  word_pairs = align_words(true[i], bad[i])\n",
        "  word_pairs = [('<start>', '<start>')] + [('<start>', '<start>')] + word_pairs\n",
        "\n",
        "  for j in range(2, len(word_pairs)):\n",
        "    pred = None\n",
        "    if word_pairs[j][1] in WORDS:\n",
        "      pred = word_pairs[j][1]\n",
        "    else:\n",
        "      predicted = pot_replacements_with_probs(pot_replacements(word_pairs[j][1]))\n",
        "\n",
        "      second_word = word_pairs[j-1][1]\n",
        "      first_word = word_pairs[j-2][1]\n",
        "      bigram = first_word + ' ' + second_word\n",
        "\n",
        "      if bigram not in bigrams:\n",
        "        if predicted:\n",
        "          pred = predicted[0][0]\n",
        "      else:\n",
        "        trigram_predicted = []\n",
        "        for word, prob in predicted:\n",
        "          trigram = bigram + ' ' + word\n",
        "          trigram_predicted.append((word, (prob)*(1+(trigrams[trigram]/bigrams[bigram]))))\n",
        "      \n",
        "        if trigram_predicted:\n",
        "          pred = sorted(trigram_predicted, key=lambda x: -x[1])[0][0]\n",
        "\n",
        "    if not pred:\n",
        "      pred = word_pairs[j][1]\n",
        "\n",
        "    if pred == word_pairs[j][0]:\n",
        "      correct += 1\n",
        "    else:\n",
        "      mistakes.append((word_pairs[j][0], word_pairs[j][1], pred))\n",
        "    total += 1\n",
        "\n",
        "    if word_pairs[j][0] == word_pairs[j][1]:\n",
        "      total_correct += 1\n",
        "      if word_pairs[j][0] !=  pred:\n",
        "        correct_broken += 1 \n",
        "    \n",
        "    else:\n",
        "      total_mistaken += 1\n",
        "      if word_pairs[j][0] == pred:\n",
        "        mistaken_fixed += 1\n",
        "\n",
        "  if not i % 50:\n",
        "    print(i)\n",
        "    print(correct/total)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "0.9333333333333333\n",
            "50\n",
            "0.8776167471819646\n",
            "100\n",
            "0.8709677419354839\n",
            "150\n",
            "0.8751584283903675\n",
            "200\n",
            "0.873977873977874\n",
            "250\n",
            "0.879544619904517\n",
            "300\n",
            "0.8756009615384616\n",
            "350\n",
            "0.8738550117770217\n",
            "400\n",
            "0.875666898631408\n",
            "450\n",
            "0.8735416666666667\n",
            "500\n",
            "0.8740713224368499\n",
            "550\n",
            "0.8749151391717583\n",
            "600\n",
            "0.8777178103315343\n",
            "650\n",
            "0.8786249823171595\n",
            "700\n",
            "0.8775590551181103\n",
            "750\n",
            "0.8771800540407763\n",
            "800\n",
            "0.8751586477443175\n",
            "850\n",
            "0.874313408723748\n",
            "900\n",
            "0.874466788543571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwUR-goQBPqx",
        "colab_type": "code",
        "outputId": "c1149367-38e1-4fa4-b6f9-4371a57f2377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8747252747252747\n",
            "0.4481964696853415\n",
            "0.061444814517055246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VllIqyeXGP2",
        "colab_type": "text"
      },
      "source": [
        "Можно отметить, что общий процент правильных слов с добавлением триграмной модели вырос, и сильно уменьшилось количество \"сломанных\" слов, но количество исправленных ошибок не изменилось"
      ]
    }
  ]
}